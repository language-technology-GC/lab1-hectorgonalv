{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet_ic')\n",
    "from nltk.corpus import wordnet, wordnet_ic\n",
    "import pandas as pd\n",
    "import csv\n",
    "import scipy.stats\n",
    "brown_ic = wordnet_ic.ic(\"ic-brown.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate the file and name the variable that will refer to the file\n",
    "partI_file = '/home/hector/Documents/Language_Technology/HW1/ws353.tsv'\n",
    "\n",
    "#read the data using pandas\n",
    "partI_file_read = pd.read_csv(partI_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordnet.synsets(\"drink\"))\n",
    "print(\"Length:\", len(wordnet.synsets(\"drink\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PATH SIMILARITY\"\"\"\n",
    "\n",
    "with open(partI_file, 'r') as tsv_file:\n",
    "    tsv_ws353 = csv.reader(tsv_file, delimiter='\\t')\n",
    "    counter_total = 0\n",
    "    counter_supported = 0\n",
    "    path_similarity_list = []\n",
    "    human_similarity_list = []\n",
    "    for row in tsv_ws353:\n",
    "        counter_total +=1\n",
    "        word1 = row[0]\n",
    "        word2 = row[1]\n",
    "        word1_sim = wordnet.synsets(word1)[0]\n",
    "        word2_sim = wordnet.synsets(word2)[0]\n",
    "        if word1_sim.pos() != word2_sim.pos():\n",
    "            continue\n",
    "        else:\n",
    "            similarity = round(word1_sim.path_similarity(word2_sim), 4)\n",
    "            counter_supported += 1\n",
    "            path_similarity_list.append(similarity)\n",
    "            human_similarity_list.append(row[2])\n",
    "    (stat, p) = scipy.stats.spearmanr(path_similarity_list, human_similarity_list)\n",
    "    print(f\"Spearman rho:\\t{stat:.4f}\\t(P = {p:.4f})\")\n",
    "    print(\"Supported pairs:\", counter_supported, '\\t', \"Total pairs:\", counter_total)\n",
    "    print('Coverage =', round((counter_supported / counter_total) * 100, 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RESNIK SIMILARITY\"\"\"\n",
    "\n",
    "with open(partI_file, 'r') as tsv_file:\n",
    "    tsv_ws353 = csv.reader(tsv_file, delimiter='\\t')\n",
    "    counter_total = 0\n",
    "    counter_supported = 0\n",
    "    resnik_similarity_list = []\n",
    "    human_similarity_list = []\n",
    "    for row in tsv_ws353:\n",
    "        counter_total +=1\n",
    "        word1 = row[0]\n",
    "        word2 = row[1]\n",
    "        word1_sim = wordnet.synsets(word1)[0]\n",
    "        word2_sim = wordnet.synsets(word2)[0]\n",
    "        if word1_sim.pos() != word2_sim.pos():\n",
    "            continue\n",
    "        else:\n",
    "            similarity = round(word1_sim.res_similarity(word2_sim, brown_ic), 4)\n",
    "            counter_supported += 1\n",
    "            resnik_similarity_list.append(similarity)\n",
    "            human_similarity_list.append(row[2])\n",
    "    (stat, p) = scipy.stats.spearmanr(resnik_similarity_list, human_similarity_list)\n",
    "    print(f\"Spearman rho:\\t{stat:.4f}\\t(P = {p:.4f})\")\n",
    "    print(\"Supported pairs:\", counter_supported, '\\t', \"Total pairs:\", counter_total)\n",
    "    print('Coverage =', round((counter_supported / counter_total) * 100, 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LEACOCK-CHODOROW SIMILARITY\"\"\"\n",
    "\n",
    "with open(partI_file, 'r') as tsv_file:\n",
    "    tsv_ws353 = csv.reader(tsv_file, delimiter='\\t')\n",
    "    counter_total = 0\n",
    "    counter_supported = 0\n",
    "    lch_similarity_list = []\n",
    "    human_similarity_list = []\n",
    "    for row in tsv_ws353:\n",
    "        counter_total +=1\n",
    "        word1 = row[0]\n",
    "        word2 = row[1]\n",
    "        word1_sim = wordnet.synsets(word1)[0]\n",
    "        word2_sim = wordnet.synsets(word2)[0]\n",
    "        if word1_sim.pos() != word2_sim.pos():\n",
    "            continue\n",
    "        else:\n",
    "            similarity = round(word1_sim.lch_similarity(word2_sim), 4)\n",
    "            counter_supported += 1\n",
    "            lch_similarity_list.append(similarity)\n",
    "            human_similarity_list.append(row[2])\n",
    "    (stat, p) = scipy.stats.spearmanr(lch_similarity_list, human_similarity_list)\n",
    "    print(f\"Spearman rho:\\t{stat:.4f}\\t(P = {p:.4f})\")\n",
    "    print(\"Supported pairs:\", counter_supported, '\\t', \"Total pairs:\", counter_total)\n",
    "    print('Coverage =', round((counter_supported / counter_total) * 100, 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WU-PALMER SIMILARITY\"\"\"\n",
    "\n",
    "with open(partI_file, 'r') as tsv_file:\n",
    "    tsv_ws353 = csv.reader(tsv_file, delimiter='\\t')\n",
    "    counter_total = 0\n",
    "    counter_supported = 0\n",
    "    wup_similarity_list = []\n",
    "    human_similarity_list = []\n",
    "    for row in tsv_ws353:\n",
    "        counter_total +=1\n",
    "        word1 = row[0]\n",
    "        word2 = row[1]\n",
    "        word1_sim = wordnet.synsets(word1)[0]\n",
    "        word2_sim = wordnet.synsets(word2)[0]\n",
    "        if word1_sim.pos() != word2_sim.pos():\n",
    "            continue\n",
    "        else:\n",
    "            similarity = round(word1_sim.wup_similarity(word2_sim), 4)\n",
    "            counter_supported += 1\n",
    "            wup_similarity_list.append(similarity)\n",
    "            human_similarity_list.append(row[2])\n",
    "    (stat, p) = scipy.stats.spearmanr(wup_similarity_list, human_similarity_list)\n",
    "    print(f\"Spearman rho:\\t{stat:.4f}\\t(P = {p:.4f})\")\n",
    "    print(\"Supported pairs:\", counter_supported, '\\t', \"Total pairs:\", counter_total)\n",
    "    print('Coverage =', round((counter_supported / counter_total) * 100, 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"JIANG-CONRATH SIMILARITY\"\"\"\n",
    "\n",
    "with open(partI_file, 'r') as tsv_file:\n",
    "    tsv_ws353 = csv.reader(tsv_file, delimiter='\\t')\n",
    "    counter_total = 0\n",
    "    counter_supported = 0\n",
    "    jcn_similarity_list = []\n",
    "    human_similarity_list = []\n",
    "    for row in tsv_ws353:\n",
    "        counter_total +=1\n",
    "        word1 = row[0]\n",
    "        word2 = row[1]\n",
    "        word1_sim = wordnet.synsets(word1)[0]\n",
    "        word2_sim = wordnet.synsets(word2)[0]\n",
    "        if word1_sim.pos() != word2_sim.pos():\n",
    "            continue\n",
    "        else:\n",
    "            similarity = round(word1_sim.jcn_similarity(word2_sim, brown_ic), 4)\n",
    "            counter_supported += 1\n",
    "            jcn_similarity_list.append(similarity)\n",
    "            human_similarity_list.append(row[2])\n",
    "    (stat, p) = scipy.stats.spearmanr(jcn_similarity_list, human_similarity_list)\n",
    "    print(f\"Spearman rho:\\t{stat:.4f}\\t(P = {p:.4f})\")\n",
    "    print(\"Supported pairs:\", counter_supported, '\\t', \"Total pairs:\", counter_total)\n",
    "    print('Coverage =', round((counter_supported / counter_total) * 100, 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LIN SIMILARITY\"\"\"\n",
    "\n",
    "with open(partI_file, 'r') as tsv_file:\n",
    "    tsv_ws353 = csv.reader(tsv_file, delimiter='\\t')\n",
    "    counter_total = 0\n",
    "    counter_supported = 0\n",
    "    lin_similarity_list = []\n",
    "    human_similarity_list = []\n",
    "    for row in tsv_ws353:\n",
    "        counter_total +=1\n",
    "        word1 = row[0]\n",
    "        word2 = row[1]\n",
    "        word1_sim = wordnet.synsets(word1)[0]\n",
    "        word2_sim = wordnet.synsets(word2)[0]\n",
    "        if word1_sim.pos() != word2_sim.pos():\n",
    "            continue\n",
    "        else:\n",
    "            similarity = round(word1_sim.lin_similarity(word2_sim, brown_ic), 4)\n",
    "            counter_supported += 1\n",
    "            lin_similarity_list.append(similarity)\n",
    "            human_similarity_list.append(row[2])\n",
    "    (stat, p) = scipy.stats.spearmanr(lin_similarity_list, human_similarity_list)\n",
    "    print(f\"Spearman rho:\\t{stat:.4f}\\t(P = {p:.4f})\")\n",
    "    print(\"Supported pairs:\", counter_supported, '\\t', \"Total pairs:\", counter_total)\n",
    "    print('Coverage =', round((counter_supported / counter_total) * 100, 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
